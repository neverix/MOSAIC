{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "# Define the datasets to load\n",
    "datasets_to_load = [\n",
    "    {\n",
    "        \"name\": \"cardiffnlp/tweet_sentiment_multilingual\",\n",
    "        \"config_name\": \"spanish\",\n",
    "        \"split\": \"test\",\n",
    "        \"text_field\": \"text\",\n",
    "        \"label_field\": \"label\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"cardiffnlp/tweet_sentiment_multilingual\",\n",
    "        \"config_name\": \"french\",\n",
    "        \"split\": \"test\",\n",
    "        \"text_field\": \"text\",\n",
    "        \"label_field\": \"label\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"cardiffnlp/tweet_sentiment_multilingual\",\n",
    "        \"config_name\": \"german\",\n",
    "        \"split\": \"test\",\n",
    "        \"text_field\": \"text\",\n",
    "        \"label_field\": \"label\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"cardiffnlp/tweet_sentiment_multilingual\",\n",
    "        \"config_name\": \"portuguese\",\n",
    "        \"split\": \"test\",\n",
    "        \"text_field\": \"text\",\n",
    "        \"label_field\": \"label\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"cardiffnlp/tweet_sentiment_multilingual\",\n",
    "        \"config_name\": \"italian\",\n",
    "        \"split\": \"test\",\n",
    "        \"text_field\": \"text\",\n",
    "        \"label_field\": \"label\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"cardiffnlp/tweet_sentiment_multilingual\",\n",
    "        \"config_name\": \"arabic\",\n",
    "        \"split\": \"test\",\n",
    "        \"text_field\": \"text\",\n",
    "        \"label_field\": \"label\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"cardiffnlp/tweet_sentiment_multilingual\",\n",
    "        \"config_name\": \"hindi\",\n",
    "        \"split\": \"test\",\n",
    "        \"text_field\": \"text\",\n",
    "        \"label_field\": \"label\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to generate the batch translation request body\n",
    "# def generate_batch_request(text, index, language):\n",
    "#     return {\n",
    "#         \"custom_id\": f\"task-{language}-{index}\",\n",
    "#         \"method\": \"POST\",\n",
    "#         \"url\": \"/v1/chat/completions\",\n",
    "#         \"body\": {\n",
    "#             \"model\": \"gpt-4o-mini\",\n",
    "#             \"messages\": [\n",
    "#                 {\n",
    "#                     \"role\": \"system\",\n",
    "#                     \"content\": \"You are a helpful assistant that translates text to English.\",\n",
    "#                 },\n",
    "#                 {\n",
    "#                     \"role\": \"user\",\n",
    "#                     \"content\": f\"Please translate the following to English and only the translation: {text}\",\n",
    "#                 },\n",
    "#             ],\n",
    "#             \"temperature\": 0.1,\n",
    "#         },\n",
    "#     }\n",
    "\n",
    "\n",
    "# # Load the datasets and generate batch requests\n",
    "# for dataset_info in datasets_to_load:\n",
    "#     dataset = load_dataset(\n",
    "#         dataset_info[\"name\"],\n",
    "#         name=dataset_info[\"config_name\"],\n",
    "#         split=dataset_info[\"split\"],\n",
    "#     )\n",
    "#     filename = f\"../translation_jobs/batch_cardiff_requests_{dataset_info['config_name']}.jsonl\"\n",
    "#     with open(filename, \"w\") as f:\n",
    "#         for i, example in enumerate(dataset):\n",
    "#             text = example[dataset_info[\"text_field\"]]\n",
    "#             language = dataset_info[\"config_name\"]\n",
    "#             request = generate_batch_request(text, i, language)\n",
    "#             f.write(json.dumps(request) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit these to openai batch api here - https://platform.openai.com/batches \n",
    "\n",
    "Then save the responses in the same the responses with the same name but swapping requests for responses \n",
    "\n",
    "e.g. as src/translation_jobs/batch_cardiff_requests_arabic.jsonl --> src/translation_jobs/batch_cardiff_responses_arabic.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb7f43a9af44447b27c2a1b699e6c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee954e04dbc49038a92b31174c99ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89bc5fdf067b43afa07b96228f6fa86a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42aed5236bec42a2a3c8fa341d76e3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5be40da0d047789341db5c2d6d8084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3acebcd7ec4b7e8f9a6c89a3c8b170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a1fe9020b34be2a911eebb8d51b97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08dfec877a82429ea6f32e7f03c410a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fedd9eafaf7047e99911c14cfc766a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9840325ef63411a9f26f486f3cf5328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b0021ee22f4180aa71efc19ac1d530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31906ef6bf74465b24de4cf4580537d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa883668165e4668874158877eafbb4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda36e388e3a4ec5943c83e3416abe4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/AIM-Harvard/cardiffnlp_tweet_sentiment_multilingual_translated/commit/f2ad118ce34b4c21d815646554ba70c09f7753d5', commit_message='Upload dataset', commit_description='', oid='f2ad118ce34b4c21d815646554ba70c09f7753d5', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "import json\n",
    "\n",
    "\n",
    "def load_translations(filename):\n",
    "    translations = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            response = json.loads(line)\n",
    "            translation = response[\"response\"][\"body\"][\"choices\"][0][\"message\"][\n",
    "                \"content\"\n",
    "            ]\n",
    "            translations.append(translation)\n",
    "    return translations\n",
    "\n",
    "\n",
    "hf_org = \"AIM-Harvard\"\n",
    "all_splits = {}\n",
    "\n",
    "for dataset_info in datasets_to_load:\n",
    "    # Load the full dataset for the given config\n",
    "    full_dataset = load_dataset(\n",
    "        dataset_info[\"name\"],\n",
    "        name=dataset_info[\"config_name\"],\n",
    "    )\n",
    "\n",
    "    # Select only the test split\n",
    "    test_dataset = full_dataset[\"test\"]\n",
    "\n",
    "    # Load translations from your JSONL file\n",
    "    translation_filename = f\"../translation_jobs/batch_cardiff_responses_{dataset_info['config_name']}.jsonl\"\n",
    "    translations = load_translations(translation_filename)\n",
    "\n",
    "    # Add the translations as a new column\n",
    "    test_dataset = test_dataset.add_column(\"translation\", translations)\n",
    "\n",
    "    # Store this test set under a key named by the language/config\n",
    "    all_splits[dataset_info[\"config_name\"]] = test_dataset\n",
    "\n",
    "# Create a DatasetDict from all the test splits\n",
    "dataset_dict = DatasetDict(all_splits)\n",
    "\n",
    "# Generate a clean name for pushing to the Hub\n",
    "cleaned_name = datasets_to_load[0][\"name\"].replace(\"/\", \"_\")\n",
    "translated_name = f\"{cleaned_name}_translated\"\n",
    "\n",
    "# Push once, containing all languages (one split per language)\n",
    "dataset_dict.push_to_hub(f\"{hf_org}/{translated_name}\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'نوال الزغبي (الشاب خالد ليس عالمي) هههههههه أتفرجي على ها الفيديو يا مبتدئة http vía @user',\n",
       " 'label': 0,\n",
       " 'translation': 'Nawal El Zoghbi (Cheb Khaled is not my world) hahahaha watch this video, you beginner http via @user'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_org = \"AIM-Harvard\"\n",
    "df = load_dataset(\n",
    "    f\"{hf_org}/cardiffnlp_tweet_sentiment_multilingual_translated\", split=\"arabic\"\n",
    ")\n",
    "df[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saefari",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
