{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def extract_f1_scores(dashboard_dir, output_csv):\n",
    "    \"\"\"\n",
    "    Traverse the dashboard directory, extract macro avg F1 scores from JSON files,\n",
    "    and save to a CSV file along with relevant metadata.\n",
    "\n",
    "    Parameters:\n",
    "        dashboard_dir (str): Base directory containing the structured dashboard outputs.\n",
    "        output_csv (str): Path to save the resulting CSV file.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "\n",
    "    # Open the CSV file for writing\n",
    "    with open(output_csv, mode=\"w\", newline=\"\") as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "\n",
    "        # Write CSV header\n",
    "        csv_writer.writerow(\n",
    "            [\n",
    "                \"timestamp\",\n",
    "                \"model_name\",\n",
    "                \"layer\",\n",
    "                \"width\",\n",
    "                \"type\",\n",
    "                \"dataset_name\",\n",
    "                \"dataset_split\",\n",
    "                \"linear_macro_f1_score\",\n",
    "                \"linear_accuracy\",\n",
    "                \"linear_mean_cv_accuracy\",\n",
    "                \"linear_std_cv_accuracy\",\n",
    "                \"decision_tree_macro_f1_score\",\n",
    "                \"decision_tree_accuracy\",\n",
    "                \"decision_tree_mean_cv_accuracy\",\n",
    "                \"decision_tree_std_cv_accuracy\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Traverse the dashboard directory\n",
    "        for root, _, files in os.walk(dashboard_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(\".json\"):\n",
    "                    json_path = os.path.join(root, file)\n",
    "\n",
    "                    try:\n",
    "                        # Load the JSON file\n",
    "                        with open(json_path, \"r\") as f:\n",
    "                            data = json.load(f)\n",
    "\n",
    "                        # Extract metadata\n",
    "                        metadata = data.get(\"metadata\", {})\n",
    "                        models = data.get(\"models\", {})\n",
    "\n",
    "                        timestamp = metadata.get(\"timestamp\", \"N/A\")\n",
    "                        model_name = metadata.get(\"model\", {}).get(\"name\", \"N/A\")\n",
    "                        layer = metadata.get(\"model\", {}).get(\"layer\", \"N/A\")\n",
    "                        width = metadata.get(\"args\", {}).get(\"width\", \"N/A\")\n",
    "                        dataset_name = metadata.get(\"dataset\", {}).get(\"name\", \"N/A\")\n",
    "                        dataset_split = metadata.get(\"args\", {}).get(\n",
    "                            \"dataset_split\", \"N/A\"\n",
    "                        )\n",
    "                        hidden = metadata.get(\"dataset\", {}).get(\"hidden\", \"N/A\")\n",
    "\n",
    "                        if hidden:\n",
    "                            hidden_value = \"Hidden States\"\n",
    "                        else:\n",
    "                            hidden_value = \"SAE Features\"\n",
    "\n",
    "                        # Extract Linear Probe metrics\n",
    "                        linear_probe = models.get(\"linearProbe\", {})\n",
    "                        linear_macro_avg = linear_probe.get(\n",
    "                            \"aggregated_metrics\", {}\n",
    "                        ).get(\"macro avg\", {})\n",
    "                        linear_f1_score = linear_macro_avg.get(\"f1_score\", \"N/A\")\n",
    "\n",
    "                        linear_performance = linear_probe.get(\"performance\", {})\n",
    "                        linear_accuracy = linear_performance.get(\"accuracy\", \"N/A\")\n",
    "                        linear_cv = linear_performance.get(\"cross_validation\", {})\n",
    "                        linear_mean_cv_accuracy = linear_cv.get(\"mean_accuracy\", \"N/A\")\n",
    "                        linear_std_cv_accuracy = linear_cv.get(\"std_accuracy\", \"N/A\")\n",
    "\n",
    "                        # Extract Decision Tree metrics\n",
    "                        decision_tree = models.get(\"decisionTree\", {})\n",
    "                        decision_tree_macro_avg = decision_tree.get(\n",
    "                            \"aggregated_metrics\", {}\n",
    "                        ).get(\"macro avg\", {})\n",
    "                        decision_tree_f1_score = decision_tree_macro_avg.get(\n",
    "                            \"f1_score\", \"N/A\"\n",
    "                        )\n",
    "\n",
    "                        decision_tree_performance = decision_tree.get(\"performance\", {})\n",
    "                        decision_tree_accuracy = decision_tree_performance.get(\n",
    "                            \"accuracy\", \"N/A\"\n",
    "                        )\n",
    "                        decision_tree_cv = decision_tree_performance.get(\n",
    "                            \"cross_validation\", {}\n",
    "                        )\n",
    "                        decision_tree_mean_cv_accuracy = decision_tree_cv.get(\n",
    "                            \"mean_accuracy\", \"N/A\"\n",
    "                        )\n",
    "                        decision_tree_std_cv_accuracy = decision_tree_cv.get(\n",
    "                            \"std_accuracy\", \"N/A\"\n",
    "                        )\n",
    "\n",
    "                        # Append extracted data to CSV\n",
    "                        csv_writer.writerow(\n",
    "                            [\n",
    "                                timestamp,\n",
    "                                model_name,\n",
    "                                layer,\n",
    "                                width,\n",
    "                                hidden_value,\n",
    "                                dataset_name,\n",
    "                                dataset_split,\n",
    "                                linear_f1_score,\n",
    "                                linear_accuracy,\n",
    "                                linear_mean_cv_accuracy,\n",
    "                                linear_std_cv_accuracy,\n",
    "                                decision_tree_f1_score,\n",
    "                                decision_tree_accuracy,\n",
    "                                decision_tree_mean_cv_accuracy,\n",
    "                                decision_tree_std_cv_accuracy,\n",
    "                            ]\n",
    "                        )\n",
    "\n",
    "                        print(f\"Processed: {json_path}\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {json_path}: {e}\")\n",
    "\n",
    "    print(f\"Extraction complete. Results saved to {output_csv}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    dashboard_dir = (\n",
    "        \"./output/probe_performance/\"  # Replace with your dashboard directory\n",
    "    )\n",
    "    output_csv = \"./macro_f1_scores.csv\"  # Replace with your desired CSV output file\n",
    "    extract_f1_scores(dashboard_dir, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "raw_df = pd.read_csv(output_csv)\n",
    "\n",
    "# Group by relevant fields to compute mean scores\n",
    "grouped_df = (\n",
    "    raw_df.groupby(\n",
    "        [\"model_name\", \"layer\", \"width\", \"type\", \"dataset_name\", \"dataset_split\"]\n",
    "    )\n",
    "    .agg(\n",
    "        {\n",
    "            \"linear_macro_f1_score\": \"mean\",\n",
    "            \"linear_accuracy\": \"mean\",\n",
    "            \"linear_mean_cv_accuracy\": \"mean\",\n",
    "            \"linear_std_cv_accuracy\": \"mean\",\n",
    "            \"decision_tree_macro_f1_score\": \"mean\",\n",
    "            \"decision_tree_accuracy\": \"mean\",\n",
    "            \"decision_tree_mean_cv_accuracy\": \"mean\",\n",
    "            \"decision_tree_std_cv_accuracy\": \"mean\",\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Pivot the data to separate SAE Features and Hidden States\n",
    "merged_df = grouped_df.pivot(\n",
    "    index=[\"model_name\", \"layer\", \"width\", \"dataset_name\", \"dataset_split\"],\n",
    "    columns=\"type\",\n",
    "    values=[\n",
    "        \"linear_macro_f1_score\",\n",
    "        \"linear_accuracy\",\n",
    "        \"linear_mean_cv_accuracy\",\n",
    "        \"linear_std_cv_accuracy\",\n",
    "        \"decision_tree_macro_f1_score\",\n",
    "        \"decision_tree_accuracy\",\n",
    "        \"decision_tree_mean_cv_accuracy\",\n",
    "        \"decision_tree_std_cv_accuracy\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Flatten the MultiIndex columns\n",
    "merged_df.columns = [\"_\".join(col).strip() for col in merged_df.columns.values]\n",
    "merged_df = merged_df.reset_index()\n",
    "\n",
    "# Preview the merged DataFrame\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use(\"seaborn-v0_8-paper\")\n",
    "\n",
    "# Use colorblind-friendly palette\n",
    "# Choose from color-blind friendly palettes\n",
    "colors = sns.color_palette(\"colorblind\")\n",
    "\n",
    "# Create settings combinations for x-axis\n",
    "settings = merged_df.apply(\n",
    "    lambda row: f\"{row['model_name']}\\n{row['width']}\\n{row['layer']}\\n{row['dataset_name']}\",\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8), sharey=True)\n",
    "\n",
    "# Set position of bars\n",
    "x = np.arange(len(settings))\n",
    "width = 0.35  # Width of the bars\n",
    "\n",
    "# Nice Colors for the bars\n",
    "colors_hidden = colors[0]\n",
    "colors_sae = colors[2]\n",
    "\n",
    "# Linear Probe Results\n",
    "probe_hidden = merged_df[\"linear_macro_f1_score_Hidden States\"].fillna(0).values\n",
    "probe_sae = merged_df[\"linear_macro_f1_score_SAE Features\"].fillna(0).values\n",
    "\n",
    "rects1 = ax1.bar(\n",
    "    x - width / 2,\n",
    "    probe_hidden,\n",
    "    width,\n",
    "    label=\"Hidden States\",\n",
    "    color=colors_hidden,\n",
    "    alpha=0.9,\n",
    ")\n",
    "rects2 = ax1.bar(\n",
    "    x + width / 2, probe_sae, width, label=\"SAE Features\", color=colors_sae, alpha=0.9\n",
    ")\n",
    "\n",
    "ax1.set_ylabel(\"Macro F1 Score\", fontsize=14)\n",
    "ax1.set_title(\"Linear Probe Performance\", fontsize=16, weight=\"bold\")\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(settings, rotation=45, ha=\"right\", fontsize=10)\n",
    "ax1.legend(fontsize=12, loc=\"upper left\")\n",
    "ax1.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Decision Tree Results\n",
    "tree_hidden = merged_df[\"decision_tree_macro_f1_score_Hidden States\"].fillna(0).values\n",
    "tree_sae = merged_df[\"decision_tree_macro_f1_score_SAE Features\"].fillna(0).values\n",
    "\n",
    "rects3 = ax2.bar(\n",
    "    x - width / 2,\n",
    "    tree_hidden,\n",
    "    width,\n",
    "    label=\"Hidden States\",\n",
    "    color=colors_hidden,\n",
    "    alpha=0.9,\n",
    ")\n",
    "rects4 = ax2.bar(\n",
    "    x + width / 2, tree_sae, width, label=\"SAE Features\", color=colors_sae, alpha=0.9\n",
    ")\n",
    "\n",
    "ax2.set_ylabel(\"Macro F1 Score\", fontsize=14)\n",
    "ax2.set_title(\"Decision Tree Performance\", fontsize=16, weight=\"bold\")\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(settings, rotation=45, ha=\"right\", fontsize=10)\n",
    "ax2.legend(fontsize=12, loc=\"upper left\")\n",
    "ax2.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "\n",
    "# Add value labels to bars\n",
    "def autolabel(ax, rects, color):\n",
    "    \"\"\"Attach a text label above each bar displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        if height > 0:  # Only annotate non-zero values\n",
    "            ax.annotate(\n",
    "                f\"{height:.2f}\",\n",
    "                xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                xytext=(0, 5),  # Vertical offset\n",
    "                textcoords=\"offset points\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=10,\n",
    "                color=color,\n",
    "                weight=\"bold\",\n",
    "            )\n",
    "\n",
    "\n",
    "autolabel(ax1, rects1, colors_hidden)\n",
    "autolabel(ax1, rects2, colors_sae)\n",
    "autolabel(ax2, rects3, colors_hidden)\n",
    "autolabel(ax2, rects4, colors_sae)\n",
    "\n",
    "# Tighten layout and add overall title\n",
    "fig.suptitle(\n",
    "    \"Macro F1 Score Comparison: Linear Probe vs. Decision Tree\",\n",
    "    fontsize=18,\n",
    "    weight=\"bold\",\n",
    ")\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"f1_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saefari",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
