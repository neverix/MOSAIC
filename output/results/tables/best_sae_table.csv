dataset_name,layer,width,top_n,binarize_value,linear_macro_f1_score
AIM-Harvard/google_gemma_2_2b_pubmed_qa,31,16k,0,1,0.7126436781609196
AIM-Harvard/google_gemma_2_9b_it_pubmed_qa,20,16k,0,0,0.6310763888888888
AIM-Harvard/google_gemma_2_9b_pubmed_qa,20,16k,50,1,0.6753246753246753
AIM-Harvard/multilingual_toxicity_dataset,12,16k,0,1,0.97
AIM-Harvard/reject_prompts,5,16k,20,0,1.0
Anthropic/election_questions,12,65k,50,0,1.0
SetFit/tweet_eval_stance_abortion,31,16k,0,0,0.7775489928406748
cardiffnlp/tweet_sentiment_multilingual,20,16k,0,0,0.8567354214413038
gallifantjack/pminervini_NQ_Swap_org_answer_None_openai_google_gemma_2_9b_it,9,16k,50,0,0.8711975016834712
gallifantjack/pminervini_NQ_Swap_org_answer_question_openai_google_gemma_2_9b_it,9,16k,50,1,0.7368706961287127
gallifantjack/pminervini_NQ_Swap_sub_answer_question_openai_google_gemma_2_9b_it,5,16k,50,1,0.6265739053987227
jackhhao/jailbreak-classification,9,16k,50,1,1.0
legacy-datasets/banking77,12,16k,0,1,0.9163701462487608
textdetox/multilingual_toxicity_dataset,9,16k,0,0,0.9719989919637106
willcb/massive-intent,20,16k,20,0,0.8306834520121563
willcb/massive-scenario,9,16k,50,0,0.9060420211075048
