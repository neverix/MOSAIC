{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils (just run this cell) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "\n",
    "\n",
    "def extract_scores(dashboard_dir, output_csv):\n",
    "    \"\"\"\n",
    "    Traverse the dashboard directory, extract macro avg F1 scores from JSON files,\n",
    "    and save to a CSV file along with relevant metadata.\n",
    "\n",
    "    Parameters:\n",
    "        dashboard_dir (str): Base directory containing the structured dashboard outputs.\n",
    "        output_csv (str): Path to save the resulting CSV file.\n",
    "    \"\"\"\n",
    "    entries_processed = 0\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "\n",
    "    # Open the CSV file for writing\n",
    "    with open(output_csv, mode=\"w\", newline=\"\") as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "\n",
    "        # Write CSV header\n",
    "        csv_writer.writerow(\n",
    "            [\n",
    "                \"timestamp\",\n",
    "                \"model_name\",\n",
    "                \"layer\",\n",
    "                \"width\",\n",
    "                \"type\",\n",
    "                \"dataset_name\",\n",
    "                \"dataset_split\",\n",
    "                \"linear_macro_f1_score\",\n",
    "                \"linear_accuracy\",\n",
    "                \"linear_mean_cv_accuracy\",\n",
    "                \"linear_std_cv_accuracy\",\n",
    "                \"decision_tree_macro_f1_score\",\n",
    "                \"decision_tree_accuracy\",\n",
    "                \"decision_tree_mean_cv_accuracy\",\n",
    "                \"decision_tree_std_cv_accuracy\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Traverse the dashboard directory\n",
    "        for root, _, files in os.walk(dashboard_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(\".json\"):\n",
    "                    json_path = os.path.join(root, file)\n",
    "\n",
    "                    try:\n",
    "                        # Load the JSON file\n",
    "                        with open(json_path, \"r\") as f:\n",
    "                            data = json.load(f)\n",
    "\n",
    "                        # Extract metadata\n",
    "                        metadata = data.get(\"metadata\", {})\n",
    "                        models = data.get(\"models\", {})\n",
    "\n",
    "                        timestamp = metadata.get(\"timestamp\", \"N/A\")\n",
    "                        model_name = metadata.get(\"model\", {}).get(\"name\", \"N/A\")\n",
    "                        layer = metadata.get(\"model\", {}).get(\"layer\", \"N/A\")\n",
    "                        width = metadata.get(\"args\", {}).get(\"width\", \"N/A\")\n",
    "                        dataset_name = metadata.get(\"dataset\", {}).get(\"name\", \"N/A\")\n",
    "                        dataset_split = metadata.get(\"args\", {}).get(\n",
    "                            \"dataset_split\", \"N/A\"\n",
    "                        )\n",
    "                        hidden = metadata.get(\"dataset\", {}).get(\"hidden\", \"N/A\")\n",
    "\n",
    "                        if hidden:\n",
    "                            hidden_value = \"Hidden States\"\n",
    "                        else:\n",
    "                            hidden_value = \"SAE Features\"\n",
    "\n",
    "                        # Extract Linear Probe metrics\n",
    "                        linear_probe = models.get(\"linearProbe\", {})\n",
    "                        linear_macro_avg = linear_probe.get(\n",
    "                            \"aggregated_metrics\", {}\n",
    "                        ).get(\"macro avg\", {})\n",
    "                        linear_f1_score = linear_macro_avg.get(\"f1_score\", \"N/A\")\n",
    "\n",
    "                        linear_performance = linear_probe.get(\"performance\", {})\n",
    "                        linear_accuracy = linear_performance.get(\"accuracy\", \"N/A\")\n",
    "                        linear_cv = linear_performance.get(\"cross_validation\", {})\n",
    "                        linear_mean_cv_accuracy = linear_cv.get(\"mean_accuracy\", \"N/A\")\n",
    "                        linear_std_cv_accuracy = linear_cv.get(\"std_accuracy\", \"N/A\")\n",
    "\n",
    "                        # Extract Decision Tree metrics\n",
    "                        decision_tree = models.get(\"decisionTree\", {})\n",
    "                        decision_tree_macro_avg = decision_tree.get(\n",
    "                            \"aggregated_metrics\", {}\n",
    "                        ).get(\"macro avg\", {})\n",
    "                        decision_tree_f1_score = decision_tree_macro_avg.get(\n",
    "                            \"f1_score\", \"N/A\"\n",
    "                        )\n",
    "\n",
    "                        decision_tree_performance = decision_tree.get(\"performance\", {})\n",
    "                        decision_tree_accuracy = decision_tree_performance.get(\n",
    "                            \"accuracy\", \"N/A\"\n",
    "                        )\n",
    "                        decision_tree_cv = decision_tree_performance.get(\n",
    "                            \"cross_validation\", {}\n",
    "                        )\n",
    "                        decision_tree_mean_cv_accuracy = decision_tree_cv.get(\n",
    "                            \"mean_accuracy\", \"N/A\"\n",
    "                        )\n",
    "                        decision_tree_std_cv_accuracy = decision_tree_cv.get(\n",
    "                            \"std_accuracy\", \"N/A\"\n",
    "                        )\n",
    "\n",
    "                        # Append extracted data to CSV\n",
    "                        csv_writer.writerow(\n",
    "                            [\n",
    "                                timestamp,\n",
    "                                model_name,\n",
    "                                layer,\n",
    "                                width,\n",
    "                                hidden_value,\n",
    "                                dataset_name,\n",
    "                                dataset_split,\n",
    "                                linear_f1_score,\n",
    "                                linear_accuracy,\n",
    "                                linear_mean_cv_accuracy,\n",
    "                                linear_std_cv_accuracy,\n",
    "                                decision_tree_f1_score,\n",
    "                                decision_tree_accuracy,\n",
    "                                decision_tree_mean_cv_accuracy,\n",
    "                                decision_tree_std_cv_accuracy,\n",
    "                            ]\n",
    "                        )\n",
    "\n",
    "                        entries_processed += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {json_path}: {e}\")\n",
    "\n",
    "    print(f\"Processed {entries_processed} entries.\")\n",
    "    print(f\"Results saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in all the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 420 entries.\n",
      "Results saved to ./output/classifications/classify_scores.csv\n"
     ]
    }
   ],
   "source": [
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    dashboard_dir = config[\"settings\"][\"base_classify_dir\"]\n",
    "\n",
    "output_csv = os.path.join(dashboard_dir, \"classify_scores.csv\")\n",
    "extract_scores(dashboard_dir, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2527cdf85c49778d16fe2850cf4806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Model Name:', options=('google/gemma-2b',), style=Descriptâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478e9a8ecf7044afbee14c8271685bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saefari",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
